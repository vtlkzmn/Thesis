{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "from random import shuffle\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similars(map_items, item_sims, map_items_reverse, item, truncate_model=200):\n",
    "    item_index = map_items[item]\n",
    "    sims = [(i, k) for i, k in enumerate(item_sims[item_index]) if i != item_index]\n",
    "    top_similar = sorted(sims, key=lambda x: x[1], reverse=True)[:truncate_model]\n",
    "\n",
    "    top_similar_items = [map_items_reverse[i[0]] for i in top_similar]\n",
    "    top_similar_scores = [i[1] for i in top_similar]\n",
    "\n",
    "    return top_similar_items, top_similar_scores\n",
    "\n",
    "\n",
    "def add_to_elasticsearch(data_str):\n",
    "    elastic_url = 'http://localhost:9200'\n",
    "    base_url = r'{0}/_bulk'.format(elastic_url)\n",
    "    response = requests.put(base_url, data=data_str)\n",
    "    if response.status_code == 400:\n",
    "        print('-- Not good... ')\n",
    "        print(response.text)\n",
    "\n",
    "\n",
    "def bulk_data(data, bulk_size=100):\n",
    "    data = list(data)\n",
    "    bulk_index = 0\n",
    "    while bulk_index < len(data):\n",
    "        bulk = data[bulk_index:bulk_index+bulk_size]\n",
    "        bulk_index += bulk_size\n",
    "        yield bulk\n",
    "\n",
    "\n",
    "class NumpyDecoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NumpyDecoder, self).default(obj)\n",
    "        \n",
    "def score_item_to_user_w2v(item_id, user_scored_items, w2v_model, w2v_score_time):\n",
    "    score = 0.0\n",
    "    w_acc = 0.0\n",
    "    for item in user_scored_items:\n",
    "        start = time()\n",
    "        item_score = w2v_model.wv.similarity(item, item_id)\n",
    "        w2v_score_time += time() - start\n",
    "        score += user_scored_items[item] * item_score\n",
    "        w_acc += abs(item_score)\n",
    "    if w_acc:\n",
    "        score = (score / w_acc) + movies_mean_rating[item_id]\n",
    "    else:\n",
    "        score = round(movies_mean_rating[item_id] * 2) / 2\n",
    "    return score\n",
    "\n",
    "def score_item_to_user_cos_sim(item_id, user_scored_items, cos_sim_matrix, cs_score_time):\n",
    "    score = 0.0\n",
    "    w_acc = 0.0\n",
    "    for item in user_scored_items:\n",
    "        start = time()\n",
    "        item1 = map_movies_reverse[item_id]\n",
    "        item2 = map_movies_reverse[item]\n",
    "        item_score = cos_sim_matrix[item1][item2]\n",
    "        cs_score_time += time() - start\n",
    "        score += user_scored_items[item] * item_score\n",
    "        w_acc += abs(item_score)\n",
    "    if w_acc:\n",
    "        score = (score / w_acc) + movies_mean_rating[item_id]\n",
    "    else:\n",
    "        score = round(movies_mean_rating[item_id] * 2) / 2\n",
    "    return score\n",
    "\n",
    "def process_user(user, user_id, test_sets, train_sets, item_user_matrices, i_u_mat_time):\n",
    "    shuffle(user)\n",
    "    len_user = len(user)\n",
    "    for test_set in test_sets:\n",
    "        test_set.append({})\n",
    "    for train_set in train_sets:\n",
    "        train_set.append([{}, {}])\n",
    "    k = 0\n",
    "    switchers = [int(len_user * x / 5) for x in range(1, 5)]\n",
    "    for i in range(len_user):\n",
    "        if i in switchers:\n",
    "            k += 1\n",
    "        test_sets[k][user_id][str(user[i][1])] = user[i][2]\n",
    "        temp_train_sets = [temp_set for h, temp_set in enumerate(train_sets) if h != k]\n",
    "        start = time()\n",
    "        temp_item_user_matrices = [mat for h, mat in enumerate(item_user_matrices) if h != k]\n",
    "        i_u_mat_time += time() - start\n",
    "        for train_set in temp_train_sets:\n",
    "            if user[i][2] >= 4:\n",
    "                train_set[user_id][0][str(user[i][1])] = user[i][2] - movies_mean_rating[str(user[i][1])]\n",
    "            elif user[i][2] <= 3:\n",
    "                train_set[user_id][1][str(user[i][1])] = user[i][2] - movies_mean_rating[str(user[i][1])]\n",
    "        start = time()\n",
    "        for i_u_mat in temp_item_user_matrices:\n",
    "            i_u_mat[map_movies_reverse[item], user_id-1] = user[i][2] - movies_mean_rating[str(user[i][1])]\n",
    "        i_u_mat_time += time() - start\n",
    "    user = []\n",
    "    \n",
    "    return user, user_id, test_sets, train_sets, item_user_matrices, i_u_mat_time\n",
    "\n",
    "\n",
    "def add_model_to_elastic(w2v_model):\n",
    "    data_list = []\n",
    "    for i, movie_id in enumerate(tqdm(movie_ids)):\n",
    "        try:\n",
    "            top = w2v_model.wv.most_similar(str(movie_id), topn=200)\n",
    "            top_similar_items, top_similar_scores = [], []\n",
    "            for each in top:\n",
    "                top_similar_items.append(each[0])\n",
    "                top_similar_scores.append(each[1])\n",
    "            movie_data = {}\n",
    "            movie_data['item_id'] = movie_id\n",
    "            movie_data['label'] = movie_titles[i]\n",
    "            movie_data['similars'] = top_similar_items\n",
    "            movie_data['score_similars'] = top_similar_scores\n",
    "            movie_data['metadata'] = {}\n",
    "            movie_data['metadata']['genres'] = movie_genres[i].split('|')\n",
    "            data_list.append(movie_data) \n",
    "        except KeyError as e:\n",
    "            pass\n",
    "    _index = 'b_thesis'\n",
    "    model_version = 'item2vec'\n",
    "    _type = 'item_model_{0}'.format(model_version)\n",
    "\n",
    "    print('- Add items and metadata to elastic ...')\n",
    "    n_total = len(data_list)\n",
    "    n_count = 0\n",
    "    print('... total elements to add: {0}'.format(n_total))\n",
    "    for bulk in bulk_data(data_list):\n",
    "        data = []\n",
    "        for doc in bulk:\n",
    "            n_count += 1\n",
    "            if n_count % (n_total / 10) == 0:\n",
    "                print('Total: {0} %'.format(int((100.0 * n_count) / n_total)))\n",
    "            index = {\"index\": {\"_index\": _index, \"_type\": _type, \"_id\": n_count}}\n",
    "            index = json.dumps(index)\n",
    "            document = json.dumps(doc, cls=NumpyDecoder)\n",
    "            data.extend([index, document])\n",
    "        # Add bulk to elastic search\n",
    "        data_str = '\\n'.join(data)\n",
    "        data_str += '\\n'\n",
    "        add_to_elasticsearch(data_str)\n",
    "    return True\n",
    "        \n",
    "# You can use this function to optimize model parameters!\n",
    "def optimize_param():\n",
    "    tested = []\n",
    "    for i in tqdm(range(5)):\n",
    "        # Place to play here, change epochss to other param tune them!\n",
    "        for epochss in tqdm([1,20,50, 100]):\n",
    "            W2VRMSE = W2VMAE =  0.0\n",
    "            w2v_model = Word2Vec(item_sentences, size=6, window=5, min_count=1, hs=1, workers=6, sg=1)\n",
    "            w2v_model.build_vocab(vocab, update=True)\n",
    "            w2v_model.train(item_sentences, total_examples=w2v_model.corpus_count, epochs=epochss)\n",
    "            w2v_model.init_sims()\n",
    "\n",
    "            w2v_final_rmse = 0.0\n",
    "            w2v_final_mae = 0.0\n",
    "            k = 0\n",
    "            for i in range(1,len(test)):\n",
    "                w2v_ratings_sum = 0.0\n",
    "                w2v_abs_sum = 0.0\n",
    "                test_user = test_1[i]\n",
    "                for item in test_user:\n",
    "                    actual = test_user[item]\n",
    "                    predicted_w2v = score_item_to_user_w2v(item, {**train_1[i][0], **train_1[i][1]}, w2v_model)\n",
    "                    w2v_abs_sum += abs(predicted_w2v - actual)\n",
    "                    w2v_ratings_sum += (predicted_w2v - actual)**2\n",
    "                w2v_user_rmse = math.sqrt(w2v_ratings_sum / len(test_user))\n",
    "                w2v_user_mae = w2v_abs_sum/len(test_user)\n",
    "                w2v_final_rmse += w2v_user_rmse\n",
    "                w2v_final_mae += w2v_user_mae\n",
    "                k+=1\n",
    "            W2VRMSE = w2v_final_rmse/k\n",
    "            W2VMAE = w2v_final_mae/k\n",
    "            tested.append((epochss,(W2VRMSE, W2VMAE)))\n",
    "    return tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '~/data/movielens/ml-1m'\n",
    "\n",
    "df_movies = pd.read_csv(os.path.join(base_dir, 'movies.dat'), delimiter='::', header=None,\n",
    "                        names=['movieId', 'title', 'genres'], engine='python')\n",
    "df_ratings = pd.read_csv(os.path.join(base_dir, 'ratings.dat'), delimiter='::', header=None,\n",
    "                         names=['userId', 'movieId', 'rating', 'timestamp'], engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = df_ratings['userId'].values\n",
    "items = df_ratings['movieId'].values\n",
    "ratings = df_ratings['rating'].values\n",
    "\n",
    "movie_ids = df_movies['movieId'].values\n",
    "movie_titles = df_movies['title'].values\n",
    "movie_genres = df_movies['genres'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3883/3883 [00:00<00:00, 189243.47it/s]\n",
      "100%|██████████| 3883/3883 [00:00<00:00, 356745.06it/s]\n"
     ]
    }
   ],
   "source": [
    "map_movies_str = {}\n",
    "for i, movie_id in enumerate(tqdm(movie_ids)):\n",
    "    map_movies_str[i] = str(movie_id)\n",
    "\n",
    "map_movies_reverse = {}\n",
    "for i, movie_id in enumerate(tqdm(movie_ids)):\n",
    "    map_movies_reverse[str(movie_id)] = i\n",
    "\n",
    "vocab = []\n",
    "movies_mean_rating = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000209/1000209 [00:00<00:00, 1016718.20it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(list(zip(df_ratings['userId'], df_ratings['movieId'], df_ratings['rating']))):\n",
    "    item = str(row[1])\n",
    "    rating = row[2]\n",
    "    if item not in movies_mean_rating:\n",
    "        movies_mean_rating[item] = (1, rating)\n",
    "    else:\n",
    "        movies_mean_rating[item] = (movies_mean_rating[item][0] + 1, movies_mean_rating[item][1] + rating)\n",
    "\n",
    "for movie_id in movie_ids:\n",
    "    movie_id = str(movie_id)\n",
    "    vocab.append([movie_id])\n",
    "    try:\n",
    "        movies_mean_rating[movie_id] = movies_mean_rating[movie_id][1] / movies_mean_rating[movie_id][0]\n",
    "    except KeyError:\n",
    "        movies_mean_rating[movie_id] = 2.5\n",
    "unique_genres = set([g for i in movie_genres for g in i.split('|')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = []\n",
    "user_id = 1\n",
    "# Test_sets are lists of dicts (user items)\n",
    "test_1, test_2, test_3, test_4, test_5 = [{}], [{}], [{}], [{}], [{}]\n",
    "\n",
    "# Train_sets are lists of users with positive and negative items\n",
    "train_1, train_2, train_3, train_4, train_5 = [[{}, {}]], [[{}, {}]], [[{}, {}]], [[{}, {}]], [[{}, {}]]\n",
    "test_sets = [test_1, test_2, test_3, test_4, test_5]\n",
    "train_sets = [train_1, train_2, train_3, train_4, train_5]\n",
    "\n",
    "item_user_matrix_1 = sparse.lil_matrix((len(movie_ids), len(users)), dtype=np.float32)\n",
    "item_user_matrix_5 = item_user_matrix_4 = item_user_matrix_3 = item_user_matrix_2 = item_user_matrix_1.copy()\n",
    "item_user_matrices = [item_user_matrix_1, item_user_matrix_2, item_user_matrix_3, item_user_matrix_4, item_user_matrix_5]\n",
    "i_u_mat_time = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000209/1000209 [00:31<00:00, 31614.87it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(list(zip(df_ratings['userId'], df_ratings['movieId'], df_ratings['rating']))):\n",
    "    new_user_id = row[0]\n",
    "    item = str(row[1])\n",
    "    rating = row[2]\n",
    "    if new_user_id != user_id:\n",
    "        user, user_id, test_sets, train_sets, item_user_matrices, i_u_mat_time = process_user(user, user_id, test_sets, train_sets, item_user_matrices, i_u_mat_time)\n",
    "    user.append(row)\n",
    "    user_id = new_user_id\n",
    "# Last user\n",
    "user, user_id, test_sets, train_sets, item_user_matrices, i_u_mat_time = process_user(user, user_id, test_sets, train_sets, item_user_matrices, i_u_mat_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "item_similarities_1 = cosine_similarity(item_user_matrix_1, item_user_matrix_1, dense_output=True)\n",
    "item_similarities_2 = cosine_similarity(item_user_matrix_2, item_user_matrix_2, dense_output=True)\n",
    "item_similarities_3 = cosine_similarity(item_user_matrix_3, item_user_matrix_3, dense_output=True)\n",
    "item_similarities_4 = cosine_similarity(item_user_matrix_4, item_user_matrix_4, dense_output=True)\n",
    "item_similarities_5 = cosine_similarity(item_user_matrix_5, item_user_matrix_5, dense_output=True)\n",
    "item_sims = [item_similarities_1, item_similarities_2, item_similarities_3, item_similarities_4, item_similarities_5]\n",
    "i_u_mat_time += time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6040 [00:00<?, ?it/s]/home/thesis/.local/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "100%|██████████| 6040/6040 [00:22<00:00, 274.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2VRMSE = 0.8888258772145207\n",
      "W2VMAE = 0.7258824059771143\n",
      "COSSIMRMSE = 1.0079839681679759\n",
      "COSSIMMAE = 0.8171102473804257\n",
      "W2V overall time = 20.086539268493652\n",
      "COSSIM overall time = 1.893298625946045\n",
      "sim_mat calculations took 24.055865049362183\n",
      "w2v model creating time took 52.614503383636475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_time, cs_time , w2v_score_time, cs_score_time, w2v_model_time, W2VRMSE, W2VMAE, COSSIMRMSE, COSSIMMAE = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "for k in range(5):\n",
    "    train = train_sets[0]\n",
    "    test = test_sets[0]\n",
    "    cos_sim_matrix = item_sims[0]\n",
    "    item_sentences = [list(item_ids.keys()) for user in train for item_ids in user][2:] #first two are empty, skipping\n",
    "    # start = time()\n",
    "    w2v_model = Word2Vec(item_sentences, size=len(unique_genres), window=7, min_count=1, hs=1, workers=6, sg=1)\n",
    "    w2v_model.build_vocab(vocab, update=True)\n",
    "    w2v_model.train(item_sentences, total_examples=w2v_model.corpus_count, epochs=22)\n",
    "    w2v_model.init_sims()\n",
    "    # w2v_model_time += time() - start\n",
    "\n",
    "    w2v_final_rmse, w2v_final_mae, cos_sim_final_rmse, cos_sim_final_mae = 0.0, 0.0, 0.0, 0.0\n",
    "    m = 0\n",
    "    for i in tqdm(range(1, len(test))):\n",
    "        # if i % 100 == 1: # using only 1% of test_set, otherwise it takes too long\n",
    "        w2v_ratings_sum, w2v_abs_sum, cos_sim_ratings_sum, cos_sim_abs_sum = 0.0, 0.0, 0.0, 0.0\n",
    "        test_user = test[i]\n",
    "        for item in test_user:\n",
    "            actual = test_user[item]\n",
    "            # startwv = time()\n",
    "            predicted_w2v = score_item_to_user_w2v(item, {**train[i][0], **train[i][1]}, w2v_model, w2v_score_time)\n",
    "            # w2v_time += time() - startwv\n",
    "            # startcs = time()\n",
    "            predicted_cos_sim = score_item_to_user_cos_sim(item, {**train[i][0], **train[i][1]}, cos_sim_matrix, cs_score_time)\n",
    "            # cs_time += time() - startcs\n",
    "            w2v_abs_sum += abs(predicted_w2v - actual)\n",
    "            w2v_ratings_sum += (predicted_w2v - actual)**2\n",
    "            cos_sim_abs_sum += abs(predicted_cos_sim - actual)\n",
    "            cos_sim_ratings_sum += (predicted_cos_sim - actual)**2\n",
    "        w2v_user_rmse = math.sqrt(w2v_ratings_sum / len(test_user))\n",
    "        w2v_user_mae = w2v_abs_sum/len(test_user)\n",
    "        w2v_final_rmse += w2v_user_rmse\n",
    "        w2v_final_mae += w2v_user_mae\n",
    "\n",
    "        cos_sim_user_rmse = math.sqrt(cos_sim_ratings_sum / len(test_user))\n",
    "        cos_sim_user_mae = cos_sim_abs_sum/len(test_user)\n",
    "        cos_sim_final_rmse += cos_sim_user_rmse\n",
    "        cos_sim_final_mae += cos_sim_user_mae\n",
    "        m+=1\n",
    "W2VRMSE += w2v_final_rmse/m\n",
    "W2VMAE += w2v_final_mae/m\n",
    "COSSIMRMSE += cos_sim_final_rmse/m\n",
    "COSSIMMAE += cos_sim_final_mae/m\n",
    "print('W2VRMSE =', W2VRMSE)\n",
    "print('W2VMAE =', W2VMAE)\n",
    "print('COSSIMRMSE =', COSSIMRMSE)\n",
    "print('COSSIMMAE =', COSSIMMAE)\n",
    "# print('W2V overall time =', w2v_time)\n",
    "# print('COSSIM overall time =', cs_time)\n",
    "# print('sim_mat calculations took', i_u_mat_time)\n",
    "# print('w2v model creating time took', w2v_model_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3883 [00:00<?, ?it/s]/home/thesis/.local/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "100%|██████████| 3883/3883 [00:02<00:00, 1555.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Add items and metadata to elastic ...\n",
      "... total elements to add: 3883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_model_to_elastic(w2v_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
